# Automated Machine Learning for Lightweight Concrete with Cs and Natural Fibres (Without PyCaret)

## üîß 1. Setup and Imports
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

## üìä 2. Data Preparation
# Experimental Data
data = {
    "Ca_Replacement(%)": [20]*6 + [30]*6 + [40]*6 + [50]*6 + [60]*6,
    "Fibre": ["CO", "HE", "BA", "KE", "JU", "FX"] * 5,
    "Stress": [
        38.0036, 36.0356, 30.7876, 26.7860, 32.7228, 33.3460,
        33.0836, 37.4296, 28.6556, 27.9340, 25.0476, 27.5240,
        28.5408, 25.4740, 26.9664, 22.3908, 24.4572, 19.6520,
        17.8316, 26.8352, 23.6208, 22.7844, 21.1116, 19.8160,
        23.7028, 24.1620, 19.1600, 22.0300, 21.8168, 20.0948
    ],
    "Split": [
        4.4347, 4.2955, 3.9084, 3.5951, 4.0540, 4.1001,
        4.0807, 4.3944, 3.7437, 3.6868, 3.4533, 3.6542,
        3.7346, 3.4884, 3.6096, 3.2286, 3.4042, 2.9855,
        2.8163, 3.5991, 3.3339, 3.2625, 3.1166, 3.0004,
        3.3408, 3.3795, 2.9404, 3.1973, 3.1787, 3.0257
    ],
    "Flexure": [
        4.3153, 4.2021, 3.8841, 3.6229, 4.0043, 4.0422,
        4.0263, 4.2826, 3.7472, 3.6997, 3.5033, 3.6724,
        3.7397, 3.5330, 3.6350, 3.3123, 3.4618, 3.1031,
        2.9559, 3.6262, 3.4021, 3.3413, 3.2163, 3.1161,
        3.4080, 3.4408, 3.0640, 3.2855, 3.2696, 3.1379
    ],
    "Sorptivity": [
        0.14, 0.13, 0.12, 0.11, 0.12, 0.15,
        0.156, 0.168, 0.144, 0.132, 0.144, 0.18,
        0.182, 0.169, 0.156, 0.143, 0.156, 0.195,
        0.196, 0.182, 0.168, 0.154, 0.168, 0.21,
        0.224, 0.208, 0.192, 0.176, 0.192, 0.24
    ]
}

df = pd.DataFrame(data)
le = LabelEncoder()
df["Fibre"] = le.fit_transform(df["Fibre"])

## ‚öôÔ∏è 3. Manual ML Pipeline per Target
models = {}
targets = ["Stress", "Split", "Flexure", "Sorptivity"]

for target in targets:
    print(f"\nTraining model for: {target}")
    X = df.drop(columns=[target])
    y = df[target]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")
    print(f"RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}")
    models[target] = model

## üìà 4. Feature Importance Plots
for target in targets:
    model = models[target]
    X = df.drop(columns=[target])
    importances = model.feature_importances_
    feat_df = pd.DataFrame({"Feature": X.columns, "Importance": importances})
    feat_df.sort_values("Importance", ascending=True, inplace=True)

    plt.figure(figsize=(6, 4))
    sns.barplot(x="Importance", y="Feature", data=feat_df)
    plt.title(f"Feature Importance for {target}")
    plt.tight_layout()
    plt.show()
